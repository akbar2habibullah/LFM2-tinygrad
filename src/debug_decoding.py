# debug_comparison.py
import json
import torch
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from tinygrad import Tensor, dtypes

from model.lfm2_modeling import LFM2ForCausalLM, LFM2Config, load_from_hf, hf_hub_download

# --- Comparison Helper ---
def compare_tensors(tg_tensor: Tensor, pt_tensor: torch.Tensor, name: str):
    """Compares a tinygrad tensor and a PyTorch tensor."""
    print(f"--- Comparing: {name} ---")
    tg_np = tg_tensor.numpy()
    # HF model might be in float16, cast to float32 for comparison
    pt_np = pt_tensor.detach().cpu().to(torch.float32).numpy()

    print(f"  Shapes: TG={tg_np.shape}, PT={pt_np.shape}")
    if tg_np.shape != pt_np.shape:
        print("  ❌ MISMATCH: Shapes are different!")
        return False

    print(f"  Means:  TG={tg_np.mean():.6f}, PT={pt_np.mean():.6f}")
    
    max_abs_diff = np.max(np.abs(tg_np - pt_np))
    print(f"  Max absolute difference: {max_abs_diff:.8f}")

    is_close = np.allclose(tg_np, pt_np, atol=1e-4, rtol=1e-3)
    if is_close:
        print("  ✅ MATCH: Tensors are numerically close.")
    else:
        print("  ❌ MISMATCH: Tensors are NOT close.")
    print("-" * (len(name) + 22))
    return is_close

def compare_caches(tg_states: list, hf_cache, config: LFM2Config, name: str) -> bool:
    """Compares the tinygrad past_states list with the Hugging Face Lfm2HybridConvCache."""
    print(f"\n--- Comparing Caches: {name} ---")
    all_match = True
    for i in range(config.num_hidden_layers):
        is_attn = i in config.full_attn_idxs
        if is_attn:
            # Attention layer: compare Key and Value caches
            tg_k, tg_v = tg_states[i]
            hf_k, hf_v = hf_cache[i] # HF cache can be indexed
            if not compare_tensors(tg_k, hf_k, f"Layer {i} Key Cache"): all_match = False
            if not compare_tensors(tg_v, hf_v, f"Layer {i} Value Cache"): all_match = False
        else:
            # Convolution layer: compare conv state
            tg_conv_state = tg_states[i]
            hf_conv_state = hf_cache.conv_cache[i]

            if not compare_tensors(tg_conv_state, hf_conv_state, f"Layer {i} Conv Cache"): all_match = False

        if not all_match:
            print(f"‼️ DIVERGENCE DETECTED IN CACHE AT LAYER {i} ‼️")
            return False
    print("--- Finished Comparing Caches ---")
    return all_match


# --- Main Comparison Logic ---
if __name__ == "__main__":
    REPO_ID = "LiquidAI/LFM2-350M"
    PROMPT = "The secret is"

    # 1. Load Hugging Face reference model
    print("Loading HF model...")
    # Use float32 to match tinygrad's default for easier comparison
    model_hf = AutoModelForCausalLM.from_pretrained(REPO_ID, torch_dtype=torch.float32).eval()
    tokenizer = AutoTokenizer.from_pretrained(REPO_ID)

    # 2. Load tinygrad model
    print("\nLoading tinygrad model...")
    config_path = hf_hub_download(repo_id=REPO_ID, filename="config.json")
    with open(config_path) as f:
        config_dict = json.load(f)
    config_tg = LFM2Config.from_hf_config(config_dict)
    model_tg = LFM2ForCausalLM(config_tg)
    load_from_hf(model_tg, REPO_ID)

    # 3. Prepare identical inputs
    input_ids_pt = tokenizer(PROMPT, return_tensors="pt")["input_ids"]
    input_ids_tg = Tensor(input_ids_pt.numpy().astype(np.int32))
    seq_len = input_ids_pt.shape[1]

    # --- 4. PREFILL STAGE COMPARISON ---
    print("\n\n--- Starting Prefill Stage Comparison ---")

    # A. Run HF model with caching enabled
    with torch.no_grad():
        pt_prefill_out = model_hf(input_ids_pt, use_cache=True, output_hidden_states=True)
        logits_pt_prefill = pt_prefill_out.logits
        cache_hf_prefill = pt_prefill_out.past_key_values

    # B. Run tinygrad model
    tg_prefill_out = model_tg(input_ids_tg, start_pos=0)
    logits_tg_prefill = tg_prefill_out.logits
    cache_tg_prefill = tg_prefill_out.past_key_values
    
    # C. Compare final logits from prefill
    # We only care about the logit for the *next* token
    if not compare_tensors(logits_tg_prefill[:, -1, :], logits_pt_prefill[:, -1, :], "Prefill - Final Logit"):
        exit()

    # D. Compare the caches generated by the prefill step
    if not compare_caches(cache_tg_prefill, cache_hf_prefill, config_tg, "Prefill Cache State"):
        exit()

    print("\n✅✅✅ Prefill stage (prompt processing) matches perfectly! ✅✅✅")


    # --- 5. DECODING STAGE COMPARISON (STEP-BY-STEP) ---
    print("\n\n--- Starting Decoding Stage Comparison ---")
    
    # Initialize variables for the loop
    current_ids_pt = input_ids_pt
    current_cache_hf = cache_hf_prefill
    
    current_ids_tg = input_ids_tg
    current_cache_tg = cache_tg_prefill
    
    current_seq_len = seq_len

    for i in range(5): # Let's check 5 generation steps
        print(f"\n\n>>>>>>>>>> DECODE STEP {i} <<<<<<<<<<")
        
        # A. Get the next token ID from the PREVIOUS step's logits (use argmax for deterministic comparison)
        next_token_pt = torch.argmax(logits_pt_prefill[:, -1, :], dim=-1).unsqueeze(0)
        next_token_tg = Tensor([[logits_tg_prefill[0, -1].argmax().item()]], dtype=dtypes.int32)
        
        # Ensure the chosen tokens are the same before proceeding
        assert next_token_pt.item() == next_token_tg.item(), f"Token choice mismatch at step {i}!"
        print(f"Generating with token ID: {next_token_pt.item()}")

        # B. Prepare inputs for the current step
        position_ids_pt = torch.tensor([[current_seq_len]], dtype=torch.long)
        
        # C. Run HF model for one step
        with torch.no_grad():
            pt_decode_out = model_hf(
                input_ids=next_token_pt, 
                past_key_values=current_cache_hf, 
                use_cache=True,
                position_ids=position_ids_pt
            )
            logits_pt_decode = pt_decode_out.logits
            cache_hf_decode = pt_decode_out.past_key_values

        # D. Run tinygrad model for one step
        tg_decode_out = model_tg(
            input_ids=next_token_tg,
            past_states=current_cache_tg,
            start_pos=current_seq_len
        )
        logits_tg_decode = tg_decode_out.logits
        cache_tg_decode = tg_decode_out.past_key_values

        # E. Compare the single-token logits
        if not compare_tensors(logits_tg_decode, logits_pt_decode, f"Decode Step {i} - Logits"):
            exit()
            
        # F. Compare the updated caches
        if not compare_caches(cache_tg_decode, cache_hf_decode, config_tg, f"Decode Step {i} - Cache"):
            exit()
            
        # G. Update variables for the next iteration
        logits_pt_prefill = logits_pt_decode # The new logits become the old logits for the next step
        current_cache_hf = cache_hf_decode
        
        logits_tg_prefill = logits_tg_decode
        current_cache_tg = cache_tg_decode
        
        current_seq_len += 1

    print("\n🎉🎉🎉 All checks passed! Prefill and 5 decode steps match perfectly. 🎉🎉🎉")
    print("The issue might be in the sampling logic (temperature, etc.) or a subtle dtype issue during generation.")